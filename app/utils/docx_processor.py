"""
DOCX íŒŒì¼ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹°
"""
import io
import re
from io import BytesIO
from docx import Document
from typing import List, Dict, Optional, Any


def normalize_key(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì •ê·œí™”í•˜ì—¬ í‚¤ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë³€í™˜í•©ë‹ˆë‹¤.
    ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜, íŠ¹ìˆ˜ë¬¸ì ì œê±° ë“±ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    if not text:
        return ""
    
    # ì•ë’¤ ê³µë°± ì œê±° ë° ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ë³€í™˜
    normalized = re.sub(r'\s+', ' ', text.strip())
    
    # ì†Œë¬¸ìë¡œ ë³€í™˜
    normalized = normalized.lower()
    
    return normalized


def extract_text_with_separated_tables(file_content: bytes) -> dict:
    """
    DOCX íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ë˜, í…Œì´ë¸” í—¤ë”ì™€ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        file_content: DOCX íŒŒì¼ì˜ ë°”ì´íŠ¸ ë‚´ìš©
        
    Returns:
        {
            'paragraphs': ë¬¸ë‹¨ë“¤ì˜ ë¦¬ìŠ¤íŠ¸,
            'table_headers': ê° í…Œì´ë¸” ì²« ë²ˆì§¸ í–‰ë“¤ì˜ ë¦¬ìŠ¤íŠ¸,
            'table_data_rows': ê° í…Œì´ë¸” ë‚˜ë¨¸ì§€ í–‰ë“¤ì˜ ë¦¬ìŠ¤íŠ¸,
            'full_text': ì „ì²´ í…ìŠ¤íŠ¸
        }
    """
    try:
        # ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë³€í™˜
        file_stream = io.BytesIO(file_content)
        
        # Document ê°ì²´ ìƒì„±
        doc = Document(file_stream)
        
        # ë¬¸ë‹¨ ì¶”ì¶œ
        paragraphs = []
        for paragraph in doc.paragraphs:
            if paragraph.text.strip():  # ë¹ˆ ì¤„ ì œì™¸
                paragraphs.append(paragraph.text.strip())
        
        # í…Œì´ë¸”ì—ì„œ í—¤ë”ì™€ ë°ì´í„° ë¶„ë¦¬ ì¶”ì¶œ
        table_headers = []  # ê° í…Œì´ë¸”ì˜ ì²« ë²ˆì§¸ í–‰(í—¤ë”)
        table_data_rows = []  # ê° í…Œì´ë¸”ì˜ ë‚˜ë¨¸ì§€ í–‰ë“¤
        all_tables = []  # ì „ì²´ í…Œì´ë¸” (ê¸°ì¡´ í˜•íƒœ)
        
        for table_idx, table in enumerate(doc.tables):
            table_text = []
            
            for row_idx, row in enumerate(table.rows):
                row_text = []
                for cell in row.cells:
                    if cell.text.strip():
                        row_text.append(cell.text.strip())
                
                if row_text:
                    row_content = " | ".join(row_text)
                    table_text.append(row_content)
                    
                    # ì²« ë²ˆì§¸ í–‰ì€ í—¤ë”ë¡œ ë¶„ë¥˜
                    if row_idx == 0:
                        table_headers.append({
                            'table_index': table_idx,
                            'content': row_content
                        })
                    else:
                        # ë‚˜ë¨¸ì§€ í–‰ë“¤ì€ ë°ì´í„° í–‰ìœ¼ë¡œ ë¶„ë¥˜
                        table_data_rows.append({
                            'table_index': table_idx,
                            'row_index': row_idx,
                            'content': row_content
                        })
            
            if table_text:
                all_tables.append("\n".join(table_text))
        
        # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
        all_text = []
        
        if paragraphs:
            all_text.append("=== ë¬¸ì„œ ë‚´ìš© ===")
            all_text.extend(paragraphs)
        
        if all_tables:
            all_text.append("\n=== í‘œ ë‚´ìš© ===")
            all_text.extend(all_tables)
        
        return {
            'paragraphs': paragraphs,
            'table_headers': table_headers,
            'table_data_rows': table_data_rows,
            'full_text': "\n".join(all_text)
        }
        
    except Exception as e:
        raise Exception(f"DOCX ë¶„ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def extract_research_user_groups(table_data_rows: list) -> dict:
    """
    í…Œì´ë¸” ë°ì´í„°ì—ì„œ ë¦¬ì„œì¹˜ ì‚¬ìš©ì ê·¸ë£¹ì„ ì¶”ì¶œí•˜ê³  ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤.
    
    FGD ë¶„ì„ í…œí”Œë¦¿ì—ì„œ ê° í…Œì´ë¸”ì€ ë™ì¼í•œ ì‚¬ìš©ì ê·¸ë£¹ë“¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ëŠ” ì¤‘ë³µëœ ê·¸ë£¹ ì •ë³´ë¥¼ ì œê±°í•˜ì—¬ ê³ ìœ í•œ ë¦¬ì„œì¹˜ ëŒ€ìƒ ê·¸ë£¹ë“¤ì„ ì‹ë³„í•©ë‹ˆë‹¤.
    
    Args:
        table_data_rows: extract_text_with_separated_tablesì—ì„œ ë°˜í™˜ëœ table_data_rows
        
    Returns:
        {
            'unique_groups': ê³ ìœ í•œ ì‚¬ìš©ì ê·¸ë£¹ë“¤,
            'group_occurrence_stats': ê° ê·¸ë£¹ì˜ ë“±ì¥ íšŸìˆ˜ í†µê³„,
            'total_unique_groups': ì´ ê³ ìœ  ê·¸ë£¹ ìˆ˜,
            'total_repeated_groups': ë°˜ë³µ ë“±ì¥í•œ ê·¸ë£¹ ìˆ˜
        }
    """
    try:
        group_count = {}
        unique_groups = []
        
        for row_data in table_data_rows:
            group_info = row_data['content'].strip()  # ì•ë’¤ í™”ì´íŠ¸ìŠ¤í˜ì´ìŠ¤ ì œê±°
            group_info = re.sub(r'\s+', ' ', group_info)  # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µí•©
            
            if group_info not in group_count:
                # ì •ë¦¬ëœ group_infoë¡œ ìƒˆë¡œìš´ row_data ìƒì„±
                cleaned_group_data = row_data.copy()
                cleaned_group_data['content'] = group_info
                
                group_count[group_info] = {
                    'count': 1,
                    'first_occurrence': cleaned_group_data
                }
                unique_groups.append(cleaned_group_data)
            else:
                group_count[group_info]['count'] += 1
        
        # ê·¸ë£¹ ë“±ì¥ íšŸìˆ˜ í†µê³„ ìƒì„±
        group_occurrence_stats = {}
        for group_info, info in group_count.items():
            if info['count'] > 1:
                group_occurrence_stats[group_info] = info['count']
        
        return {
            'unique_groups': unique_groups,
            'group_occurrence_stats': group_occurrence_stats,
            'total_unique_groups': len(unique_groups),
            'total_repeated_groups': len(group_occurrence_stats)
        }
        
    except Exception as e:
        raise Exception(f"ë¦¬ì„œì¹˜ ì‚¬ìš©ì ê·¸ë£¹ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def extract_table_headers_with_subitems(file_content: bytes) -> List[Dict]:
    """
    DOCX íŒŒì¼ì—ì„œ í…Œì´ë¸” í—¤ë”ì™€ í•´ë‹¹ í…Œì´ë¸”ì˜ ì„¸ë¶€ í•­ëª©ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. (ë””ë²„ê¹… ëª¨ë“œ)
    """
    print("â¡ï¸ í•¨ìˆ˜ 'extract_table_headers_with_subitems' ì‹¤í–‰ ì‹œì‘")
    
    try:
        if not isinstance(file_content, bytes):
            print(f"âŒ ì˜¤ë¥˜: ì…ë ¥ëœ file_contentê°€ bytes íƒ€ì…ì´ ì•„ë‹™ë‹ˆë‹¤. (íƒ€ì…: {type(file_content)})")
            raise TypeError("a bytes-like object is required, not 'str'")

        print(f"   - ì…ë ¥ëœ íŒŒì¼ í¬ê¸°: {len(file_content)} bytes")
        file_stream = io.BytesIO(file_content)
        
        doc = Document(file_stream)
        print("   ğŸ“„ DOCX íŒŒì¼ ë¡œë“œ ì„±ê³µ")
        
        structured_items = []
        print(f"   - ë¬¸ì„œì—ì„œ ì´ {len(doc.tables)}ê°œì˜ í…Œì´ë¸” ë°œê²¬")
        
        for table_idx, table in enumerate(doc.tables):
            print(f"\nğŸ” {table_idx}ë²ˆ í…Œì´ë¸” ì²˜ë¦¬ ì¤‘...")
            if len(table.rows) == 0:
                print("   - í…Œì´ë¸”ì— í–‰ì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
                
            header_row = table.rows[0]
            header_text = ""
            
            # í—¤ë” í…ìŠ¤íŠ¸ í›„ë³´ë“¤ì„ ëª¨ë‘ í™•ì¸
            header_candidates = [cell.text.strip() for cell in header_row.cells]
            print(f"   - í—¤ë” í–‰ í›„ë³´ í…ìŠ¤íŠ¸: {header_candidates}")

            for cell_text in header_candidates:
                if cell_text:
                    header_text = cell_text
                    print(f"   - í…Œì´ë¸” í—¤ë”ë¥¼ '{header_text}'ë¡œ í™•ì •")
                    break
            
            if not header_text:
                print("   - ìœ íš¨í•œ í—¤ë”ë¥¼ ì°¾ì§€ ëª»í•´ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
                
            subitems = []
            
            for row_idx, row in enumerate(table.rows[1:], start=1):
                for col_idx, cell in enumerate(row.cells):
                    cell_text = cell.text.strip()
                    if not cell_text:
                        continue
                    
                    # print(f"     - [í–‰:{row_idx}, ì—´:{col_idx}] ì…€ ë‚´ìš© í™•ì¸: \"{cell_text[:30]}...\"")
                    lines = cell_text.split('\n')
                    for line in lines:
                        line = line.strip()
                        
                        if not line or len(line) < 2: continue
                        if re.match(r'^\d+-\d+$', line): continue
                        if len(line.split()) == 1 and len(line) < 10: continue
                        
                        item_text = None
                        if line.startswith('- '):
                            item_text = line[2:].strip()
                        elif line.startswith('â€¢ '):
                            item_text = line[2:].strip()
                        elif re.match(r'^\d+[\.\)]\s+', line):
                            item_text = re.sub(r'^\d+[\.\)]\s+', '', line).strip()
                        elif (len(line) > 10 and len(line) < 200 and not re.match(r'^\d+$', line) and '|' not in line):
                            item_text = line
                        
                        if item_text and item_text not in subitems and len(item_text) < 200:
                            print(f"    âœ”ï¸ ['{item_text}'] í•­ëª© ì¶”ê°€")
                            subitems.append(item_text)

            print(f"   - '{header_text}' í—¤ë”ì— ì´ {len(subitems)}ê°œì˜ ì„¸ë¶€ í•­ëª© ì¶”ì¶œ ì™„ë£Œ.")
            structured_items.append({
                'header': header_text,
                'subitems': subitems,
                'table_index': table_idx
            })
        
        print(f"\nğŸ í•¨ìˆ˜ ì‹¤í–‰ ì™„ë£Œ. ì´ {len(structured_items)}ê°œì˜ êµ¬ì¡°í™”ëœ í•­ëª© ë°˜í™˜.")
        return structured_items
        
    except Exception as e:
        print(f"âŒ í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        # ì›ë˜ ì˜¤ë¥˜ë¥¼ í¬í•¨í•˜ì—¬ ìƒˆë¡œìš´ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œì¼œ, ì–´ë””ì„œ ë¬¸ì œê°€ ìƒê²¼ëŠ”ì§€ ì¶”ì í•˜ê¸° ì‰½ê²Œ í•¨
        raise Exception(f"í…Œì´ë¸” í—¤ë” ë° ì„¸ë¶€ í•­ëª© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def format_items_for_prompt(structured_items: List[Dict]) -> str:
    """
    êµ¬ì¡°í™”ëœ í…Œì´ë¸” ì•„ì´í…œë“¤ì„ í”„ë¡¬í”„íŠ¸ìš© ê³„ì¸µì  ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    
    Args:
        structured_items: extract_table_headers_with_subitemsì—ì„œ ë°˜í™˜ëœ êµ¬ì¡°í™”ëœ ì•„ì´í…œë“¤
        
    Returns:
        í”„ë¡¬í”„íŠ¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê³„ì¸µì  êµ¬ì¡°ì˜ ë¬¸ìì—´
        ì˜ˆ:
        1. í¬ë¦¬ì—ì´í„° í˜„í™©
          - í™œë™ í˜„í™©
          - í™œë™ ëª©í‘œ
        2. ë¼ì´ë¸Œ ì´ìš© í˜„í™©
          - ë¼ì´ë¸Œ í˜„í™©
          - ê°€ë¡œ/ì„¸ë¡œ
    """
    try:
        formatted_lines = []
        
        for idx, item in enumerate(structured_items, 1):
            header = item['header']
            subitems = item['subitems']
            
            # í—¤ë”ë¥¼ ë²ˆí˜¸ì™€ í•¨ê»˜ ì¶”ê°€
            formatted_lines.append(f"{idx}. {header}")
            
            # ì„¸ë¶€ í•­ëª©ë“¤ì„ ë“¤ì—¬ì“°ê¸°ì™€ í•¨ê»˜ ì¶”ê°€
            if subitems:
                for subitem in subitems:
                    formatted_lines.append(f"  - {subitem}")
        
        return '\n'.join(formatted_lines)
        
    except Exception as e:
        raise Exception(f"í”„ë¡¬í”„íŠ¸ìš© ì•„ì´í…œ í¬ë§·íŒ… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def parse_analysis_sections_any(analysis_text: str) -> Dict[str, str]:
    """
    'analysis' ë¬¸ìì—´ì—ì„œ ë²ˆí˜¸ ì„¹ì…˜(### 1. ...)ë³„ ë³¸ë¬¸ì„ ì¶”ì¶œí•´
    { '1. ì œëª©': 'ë³¸ë¬¸', ... } í˜•íƒœë¡œ ë°˜í™˜.
    """
    text = analysis_text.replace("\r\n", "\n").replace("\r", "\n")
    sec_pat = re.compile(r'(?m)^###\s*(\d+)\.\s*(.+?)\s*$')
    sections = list(sec_pat.finditer(text))

    out: Dict[str, str] = {}
    for i, m in enumerate(sections):
        num = m.group(1)
        title = m.group(2).strip()
        start = m.end()
        end = sections[i+1].start() if i+1 < len(sections) else len(text)
        body = text[start:end].strip()
        out[f"{num}. {title}"] = body
    return out

def replace_analysis_with_parsed(job_result: Dict[str, Any]) -> Dict[str, Any]:
    """
    job_result ë‚´ ëª¨ë“  íŒŒì¼ì— ëŒ€í•´:
      - 'analysis'ê°€ ë¬¸ìì—´ì´ë©´ ì„¹ì…˜ íŒŒì‹±í•˜ê³ 
      - íŒŒì‹± ì„±ê³µ ì‹œ 'analysis'ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ êµì²´
      - íŒŒì‹± ì‹¤íŒ¨/ì„¹ì…˜ ë¯¸ì¡´ì¬ ì‹œ ì›ë¬¸ ìœ ì§€
    ìµœì¢…ì ìœ¼ë¡œ ìˆ˜ì •ëœ job_result ë°˜í™˜.
    """
    results = job_result.get("results") or {}
    for file_key, file_obj in results.items():
        analysis_text = file_obj.get("analysis")
        if isinstance(analysis_text, str) and analysis_text.strip():
            parsed = parse_analysis_sections_any(analysis_text)
            if parsed:  # ì„¹ì…˜ì„ í•˜ë‚˜ë¼ë„ ì°¾ì•˜ì„ ë•Œë§Œ êµì²´
                file_obj["analysis"] = parsed
            # else: ì„¹ì…˜ íŒ¨í„´ì´ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë‘ 
    return job_result


def fill_frame_with_analysis_bytes(json_data: dict, frame_docx_bytes: bytes) -> bytes:
    """
    JSON ê°ì²´ì™€ DOCX bytesë¥¼ ë°›ì•„, ë¶„ì„ ë‚´ìš©ì„ DOCXì— ì±„ì›Œ ë„£ê³ 
    ìˆ˜ì •ëœ DOCXë¥¼ bytesë¡œ ë°˜í™˜
    """
    results = json_data.get("results", {})
    if not results:
        print("âš ï¸ JSON ì•ˆì— resultsê°€ ì—†ìŠµë‹ˆë‹¤.")
        return frame_docx_bytes

    # group -> {header -> ë¶„ì„ë‚´ìš©} ë§¤í•‘ ìƒì„± (í‚¤ ëª¨ë‘ normalize)
    group_to_analysis: Dict[str, Dict[str, str]] = {}
    for obj in results.values():
        group = normalize_key(obj.get("group"))
        analysis = obj.get("analysis")
        if isinstance(analysis, dict):
            norm_analysis = {normalize_key(k): (v or "") for k, v in analysis.items()}
            group_to_analysis[group] = norm_analysis

    if not group_to_analysis:
        print("âš ï¸ (group, analysis dict) ë§¤í•‘ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return frame_docx_bytes

    # DOCX ë¡œë“œ (bytes â†’ Document)
    doc = Document(BytesIO(frame_docx_bytes))
    filled_count = 0

    # í‘œ ìˆœíšŒ
    for table in doc.tables:
        if not table.rows or len(table.columns) < 2:
            continue

        # í—¤ë”(ì²« í–‰) ì •ê·œí™”
        headers = [normalize_key(cell.text) for cell in table.rows[0].cells]

        # ë°ì´í„° í–‰
        for row in table.rows[1:]:
            group_name_norm = normalize_key(row.cells[0].text)
            if group_name_norm not in group_to_analysis:
                continue
            item_to_result = group_to_analysis[group_name_norm]

            # ê° ì—´ ì±„ìš°ê¸°
            for col_idx in range(1, len(row.cells)):
                header_norm = headers[col_idx]
                if header_norm in item_to_result:
                    analysis_text = item_to_result[header_norm].strip()
                    if analysis_text:
                        cell = row.cells[col_idx]
                        if cell.text.strip():
                            cell.add_paragraph("")
                        p = cell.add_paragraph()
                        run = p.add_run("[ë¶„ì„]")
                        run.bold = True
                        cell.add_paragraph(analysis_text)
                        filled_count += 1

    # ìˆ˜ì •ëœ DOCX â†’ bytes ë³€í™˜
    output_stream = BytesIO()
    doc.save(output_stream)
    return output_stream.getvalue()

